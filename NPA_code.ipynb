{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "with open('ClickData4.tsv') as f:\n",
    "    user=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "def timeconvert(timestr):\n",
    "    ifpm=False\n",
    "    if 'PM' in timestr:\n",
    "        ifpm=True\n",
    "    tp=datetime.datetime.strptime(timestr[:-3], \"%m/%d/%Y %H:%M:%S\")\n",
    "    tp+=datetime.timedelta(hours=12)\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_dict={}\n",
    "for i in user:\n",
    "    tr=i.strip().split('\\t')\n",
    "    userid=tr[0]\n",
    "    if userid not in  userid_dict:\n",
    "        userid_dict[userid]=len(userid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('DocMeta3.tsv') as f:\n",
    "    data=f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "news={}\n",
    "category={}\n",
    "subcategory={}\n",
    "for i in data:\n",
    "    tp=i.strip().split('\\t')\n",
    "    news[tp[1]]=[tp[2],tp[3],word_tokenize(tp[6].lower())]\n",
    "    category[tp[2]]=0\n",
    "    subcategory[tp[3]]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict0={'PADDING':[0,999999]}\n",
    "\n",
    "for i in news:\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict0:\n",
    "            word_dict0[j][1]+=1\n",
    "        else:\n",
    "            word_dict0[j]=[len(word_dict0),1]\n",
    "word_dict={}\n",
    "for i in word_dict0:\n",
    "    if word_dict0[i][1]>=2:\n",
    "        word_dict[i]=[len(word_dict),word_dict0[i][1]]\n",
    "print(len(word_dict),len(word_dict0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embdict={}\n",
    "nb=0\n",
    "import pickle\n",
    "with open('/home/yourpath/glove.840B.300d.txt','rb')as f:\n",
    "    linenb=0\n",
    "    while True:\n",
    "        j=f.readline()\n",
    "        if len(j)==0:\n",
    "            break\n",
    "        k = j.split()\n",
    "        word=k[0].decode()\n",
    "        linenb+=1\n",
    "        if len(word) != 0:\n",
    "            tp=[float(x) for x in k[1:]]\n",
    "            if word in word_dict:\n",
    "                embdict[word]=tp\n",
    "                if nb%100==0:\n",
    "                    print(nb,linenb,word)\n",
    "                nb+=1\n",
    "\n",
    "from numpy.linalg import cholesky\n",
    "word_dict1=word_dict\n",
    "print(len(embdict),len(word_dict1))\n",
    "print(len(word_dict1))\n",
    "lister=[0]*len(word_dict1)\n",
    "xp=np.zeros(300,dtype='float32')\n",
    "\n",
    "cand=[]\n",
    "for i in embdict.keys():\n",
    "    lister[word_dict1[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    cand.append(lister[word_dict1[i][0]])\n",
    "cand=np.array(cand,dtype='float32')\n",
    "\n",
    "mu=np.mean(cand, axis=0)\n",
    "Sigma=np.cov(cand.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(lister)):\n",
    "    if type(lister[i])==int:\n",
    "        lister[i]=np.reshape(norm, 300)\n",
    "lister[0]=np.zeros(300,dtype='float32')\n",
    "lister=np.array(lister,dtype='float32')\n",
    "print(lister.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_words=[[0]*30]\n",
    "news_index={'0':0}\n",
    "for i in news:\n",
    "    tp=[]\n",
    "    news_index[i]=len(news_index)\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict:\n",
    "            tp.append(word_dict[j][0])\n",
    "    tp=tp[:30]\n",
    "    news_words.append(tp+[0]*(30-len(tp)))\n",
    "    \n",
    "import numpy as np\n",
    "news_words=np.array(news_words,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "npratio=4\n",
    "all_train_id=[]\n",
    "all_train_pn=[]    \n",
    "all_labeler=[]\n",
    "\n",
    "all_test_id=[]\n",
    "all_test_pn=[]    \n",
    "all_test_labeler=[]\n",
    "all_test_index=[]\n",
    "\n",
    "all_user_pos=[]\n",
    "all_test_user_pos=[]\n",
    "\n",
    "for raw in user:\n",
    "    tr=raw.strip().split('\\t')\n",
    "    userid=tr[0]\n",
    "    if len(tr)==4:\n",
    "        \n",
    "        tp=[x.split('#TAB#') for x in tr[2].split('#N#')]\n",
    "    if len(tr)==3:\n",
    "        tp=[x.split('#TAB#') for x in tr[2].split('#N#')]\n",
    "\n",
    "    trainpos=[x[0].split() for x in tp]\n",
    "    trainneg=[x[1].split() for x in tp]\n",
    "     \n",
    "    ppp=list(itertools.chain(*(trainpos)))\n",
    "    nnn=list(itertools.chain(*(trainneg)))\n",
    "    #print(ppp)\n",
    "    \n",
    "    \n",
    "    if len(tr)==4:\n",
    "        tp=[x.split('#TAB#') for x in tr[3].split('#N#')]\n",
    "        testpos=[x[0].split() for x in tp]\n",
    "        testneg=[x[1].split() for x in tp]\n",
    "        \n",
    "        \n",
    "        for i in range(len(testpos)):\n",
    "            tp=[]\n",
    "            tp.append(len(all_test_pn))\n",
    "            qqq=list(set(ppp))\n",
    "            allpos=[int(p) for p in random.sample(qqq,min(50,len(qqq)))[:50]]\n",
    "            allpos+=[0]*(50-len(allpos))\n",
    "    \n",
    "            \n",
    "            for j in testpos[i]:\n",
    "                all_test_pn.append(int(j))\n",
    "                all_test_labeler.append(1)\n",
    "                all_test_id.append(userid_dict[userid])\n",
    "                all_test_user_pos.append(allpos)\n",
    "                \n",
    "            for j in testneg[i]:\n",
    "                all_test_pn.append(int(j))\n",
    "                all_test_labeler.append(0)\n",
    "                all_test_id.append(userid_dict[userid])\n",
    "                all_test_user_pos.append(allpos)\n",
    "            tp.append(len(all_test_pn))\n",
    "            all_test_index.append(tp)\n",
    "            \n",
    "    #nnnnn=list(set(nnn)-set(ppp))\n",
    "\n",
    "            \n",
    "    for mp in range(len(trainpos)):\n",
    "        for ps in trainpos[mp]:\n",
    "            #tql=list(set(nnn)-set(trainpos[mp]))\n",
    "            negps=newsample(trainneg[mp],npratio)\n",
    "            negps.append(ps)\n",
    "            tplb=[0]*npratio+[1]\n",
    "            tid=list(range(npratio+1))\n",
    "            random.shuffle(tid)\n",
    "\n",
    "            \n",
    "            yp=[]\n",
    "            yplb=[]\n",
    "            for j in tid:\n",
    "                yp.append(int(negps[j]))\n",
    "                yplb.append(tplb[j])\n",
    "            \n",
    "            qqq=list(set(ppp)-set([ps]))\n",
    "            allpos=[int(p) for p in random.sample(qqq,min(50,len(qqq)))[:50]]\n",
    "            allpos+=[0]*(50-len(allpos))\n",
    "            all_train_pn.append(yp)\n",
    "            all_labeler.append(yplb)\n",
    "            all_train_id.append(userid_dict[userid])\n",
    "            all_user_pos.append(allpos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data_random(all_train_pn,all_labeler,all_train_id,batch_size):\n",
    "    idx = np.arange(len(all_labeler))\n",
    "    np.random.shuffle(idx)\n",
    "    y=all_labeler\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            itx = news_words[all_train_pn[i]]\n",
    "            tkk=all_user_pos[all_train_id[i]]\n",
    "            itxx=[itx[:,k,:] for k in range(itx.shape[1])]\n",
    "            usx=news_words[all_user_pos[i]]\n",
    "            usxx=[usx[:,k,:] for k in range(usx.shape[1])]\n",
    "            uid=np.expand_dims(all_train_id[i],axis=1)\n",
    "            yy=all_labeler[i]\n",
    "            yield (itxx +usxx+[uid], yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data(all_train_pn,all_labeler,all_train_id,batch_size):\n",
    "    idx = np.arange(len(all_labeler))\n",
    "    y=all_labeler\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            itx = news_words[all_train_pn[i]]\n",
    "            usx=news_words[all_test_user_pos[i]]\n",
    "            usxx=[usx[:,k,:] for k in range(usx.shape[1])]\n",
    "            uid=np.expand_dims(all_train_id[i],axis=1)\n",
    "            yy=all_labeler[i]\n",
    "            yield ([itx]+ usxx+[uid], yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding, concatenate\n",
    "from keras.layers import Dense, Input, Flatten, average,Lambda\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers #keras2\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.optimizers import *\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(padding=\"same\", activation=\"relu\", strides=1, filters=400, kernel_size=3)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2a1085d844f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mtraingen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_batch_data_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_pn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_labeler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_train_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraingen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;31m#for ep in range(1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mtestgen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_batch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_test_pn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_test_labeler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_test_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     \u001b[0;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/wuch15/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/wuch15/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \"\"\"\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-6e8350de90b4>\u001b[0m in \u001b[0;36mgenerate_batch_data_random\u001b[0;34m(all_train_pn, all_labeler, all_train_id, batch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mitx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_train_pn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mtkk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_user_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_train_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mitxx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "results=[]\n",
    "for npratio in range(4,5):\n",
    "    \n",
    "    \n",
    "    MAX_SENT_LENGTH=30\n",
    "    MAX_SENTS=50\n",
    "    \n",
    "    \n",
    "    user_id = Input(shape=(1,), dtype='int32')\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    user_embedding_layer= Embedding(len(user), 50,trainable=True)\n",
    "    \n",
    "    user_embedding= user_embedding_layer(user_id)\n",
    "    \n",
    "    user_embedding0= Dense(200,activation='relu')(user_embedding)\n",
    "    user_embedding0= Flatten()(user_embedding0)\n",
    "    \n",
    "    user_embedding1= Dense(200,activation='relu')(user_embedding)\n",
    "    user_embedding1= Flatten()(user_embedding1)\n",
    "    \n",
    "    \n",
    "    #user_embedding= RepeatVector(1+npratio)(user_embedding0)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    \n",
    "    \n",
    "    embedding_layer = Embedding(33050 , 300, weights=[lister],trainable=True)\n",
    "    \n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    d_et=Dropout(0.2)(embedded_sequences)\n",
    "    \n",
    "    l_cnnt = Convolution1D(nb_filter=400, filter_length=3,  padding='same', activation='relu', strides=1)(d_et)\n",
    "    d_ct=Dropout(0.2)(l_cnnt)\n",
    "    \n",
    "    attention = Dense(200,activation='tanh')(d_ct)\n",
    "    attention = Dot((2, 1))([attention, user_embedding0])\n",
    "    attention_weight = Activation('softmax')(attention)\n",
    "    l_attt=keras.layers.Dot((1, 1))([d_ct, attention_weight])\n",
    "    \n",
    "    \n",
    "    sentEncodert = Model([sentence_input,user_id], l_attt)\n",
    "    \n",
    "    \n",
    "    review_input = [keras.Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(MAX_SENTS)]\n",
    "    candidate_vecssent = [sentEncodert([review,user_id]) for review in review_input]\n",
    "    candidate_vecssent2 =concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(review) for review in candidate_vecssent],axis=1)\n",
    "    #review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "    #review_encoder = TimeDistributed(sentEncodert)(review_input)\n",
    "    #print(K.int_shape(candidate_vecssent2))\n",
    "    \n",
    "    \n",
    "    attention2 = Dense(200,activation='tanh')(candidate_vecssent2)\n",
    "    attention2 = keras.layers.Dot((2, 1))([attention2, user_embedding1])\n",
    "    attention_weight2 = Activation('softmax')(attention2)\n",
    "    l_att2=keras.layers.Dot((1, 1))([candidate_vecssent2, attention_weight2])\n",
    "    #l_att2=AttLayer()(candidate_vecssent2)\n",
    "    \n",
    "    candidates = [keras.Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(1+npratio)]\n",
    "    candidate_vecs = [sentEncodert([candidate,user_id]) for candidate in candidates]\n",
    "    \n",
    "    \n",
    "    logits = [keras.layers.dot([l_att2, candidate_vec], axes=-1) for candidate_vec in candidate_vecs]\n",
    "    logits = keras.layers.Activation(keras.activations.softmax)(keras.layers.concatenate(logits))\n",
    "    \n",
    "    \n",
    "    model = Model(candidates+review_input+[user_id], logits)\n",
    "    \n",
    "    \n",
    "    candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "    candidate_one_vec = sentEncodert([candidate_one,user_id])\n",
    "    score = keras.layers.Activation(keras.activations.sigmoid)(\n",
    "        keras.layers.dot([l_att2, candidate_one_vec], axes=-1))\n",
    "    model2 = keras.Model([candidate_one]+review_input+[user_id], score)\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "\n",
    "    for ep in range(3):\n",
    "        traingen=generate_batch_data_random(all_train_pn,all_labeler,all_train_id, 100)\n",
    "        \n",
    "        model.fit_generator(traingen, epochs=1,steps_per_epoch=len(all_train_id)//100)\n",
    "        #for ep in range(1):    \n",
    "        testgen=generate_batch_data(all_test_pn,all_test_labeler,all_test_id, 100)\n",
    "        cr = model2.predict_generator(testgen, steps=len(all_test_id)//100,verbose=1)\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        all_auc=[]\n",
    "        all_mrr=[]\n",
    "        all_ndcg=[]\n",
    "        all_ndcg2=[]\n",
    "        for m in all_test_index:\n",
    "            if np.sum(all_test_labeler[m[0]:m[1]])!=0 and m[1]<len(cr):\n",
    "        \n",
    "                all_auc.append(roc_auc_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0]))\n",
    "                all_mrr.append(mrr_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0]))\n",
    "                all_ndcg.append(ndcg_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0],k=5))\n",
    "                all_ndcg2.append(ndcg_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0],k=10))\n",
    "        results.append([np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2)])\n",
    "        print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
