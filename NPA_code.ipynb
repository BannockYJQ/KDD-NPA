{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "with open('ClickData4.tsv') as f:\n",
    "    user=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "def timeconvert(timestr):\n",
    "    ifpm=False\n",
    "    if 'PM' in timestr:\n",
    "        ifpm=True\n",
    "    tp=datetime.datetime.strptime(timestr[:-3], \"%m/%d/%Y %H:%M:%S\")\n",
    "    tp+=datetime.timedelta(hours=12)\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_dict={}\n",
    "for i in user:\n",
    "    tr=i.strip().split('\\t')\n",
    "    userid=tr[0]\n",
    "    if userid not in  userid_dict:\n",
    "        userid_dict[userid]=len(userid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('DocMeta3.tsv') as f:\n",
    "    data=f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "news={}\n",
    "category={}\n",
    "subcategory={}\n",
    "for i in data:\n",
    "    tp=i.strip().split('\\t')\n",
    "    news[tp[1]]=[tp[2],tp[3],word_tokenize(tp[6].lower())]\n",
    "    category[tp[2]]=0\n",
    "    subcategory[tp[3]]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict0={'PADDING':[0,999999]}\n",
    "\n",
    "for i in news:\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict0:\n",
    "            word_dict0[j][1]+=1\n",
    "        else:\n",
    "            word_dict0[j]=[len(word_dict0),1]\n",
    "word_dict={}\n",
    "for i in word_dict0:\n",
    "    if word_dict0[i][1]>=2:\n",
    "        word_dict[i]=[len(word_dict),word_dict0[i][1]]\n",
    "print(len(word_dict),len(word_dict0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embdict={}\n",
    "nb=0\n",
    "import pickle\n",
    "with open('/home/yourpath/glove.840B.300d.txt','rb')as f:\n",
    "    linenb=0\n",
    "    while True:\n",
    "        j=f.readline()\n",
    "        if len(j)==0:\n",
    "            break\n",
    "        k = j.split()\n",
    "        word=k[0].decode()\n",
    "        linenb+=1\n",
    "        if len(word) != 0:\n",
    "            tp=[float(x) for x in k[1:]]\n",
    "            if word in word_dict:\n",
    "                embdict[word]=tp\n",
    "                if nb%100==0:\n",
    "                    print(nb,linenb,word)\n",
    "                nb+=1\n",
    "\n",
    "from numpy.linalg import cholesky\n",
    "word_dict1=word_dict\n",
    "print(len(embdict),len(word_dict1))\n",
    "print(len(word_dict1))\n",
    "lister=[0]*len(word_dict1)\n",
    "xp=np.zeros(300,dtype='float32')\n",
    "\n",
    "cand=[]\n",
    "for i in embdict.keys():\n",
    "    lister[word_dict1[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    cand.append(lister[word_dict1[i][0]])\n",
    "cand=np.array(cand,dtype='float32')\n",
    "\n",
    "mu=np.mean(cand, axis=0)\n",
    "Sigma=np.cov(cand.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(lister)):\n",
    "    if type(lister[i])==int:\n",
    "        lister[i]=np.reshape(norm, 300)\n",
    "lister[0]=np.zeros(300,dtype='float32')\n",
    "lister=np.array(lister,dtype='float32')\n",
    "print(lister.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_words=[[0]*30]\n",
    "news_index={'0':0}\n",
    "for i in news:\n",
    "    tp=[]\n",
    "    news_index[i]=len(news_index)\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict:\n",
    "            tp.append(word_dict[j][0])\n",
    "    tp=tp[:30]\n",
    "    news_words.append(tp+[0]*(30-len(tp)))\n",
    "    \n",
    "import numpy as np\n",
    "news_words=np.array(news_words,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "npratio=4\n",
    "all_train_id=[]\n",
    "all_train_pn=[]    \n",
    "all_labeler=[]\n",
    "\n",
    "all_test_id=[]\n",
    "all_test_pn=[]    \n",
    "all_test_labeler=[]\n",
    "all_test_index=[]\n",
    "\n",
    "all_user_pos=[]\n",
    "all_test_user_pos=[]\n",
    "\n",
    "for raw in user:\n",
    "    tr=raw.strip().split('\\t')\n",
    "    userid=tr[0]\n",
    "    if len(tr)==4:\n",
    "        \n",
    "        tp=[x.split('#TAB#') for x in tr[2].split('#N#')]\n",
    "    if len(tr)==3:\n",
    "        tp=[x.split('#TAB#') for x in tr[2].split('#N#')]\n",
    "\n",
    "    trainpos=[x[0].split() for x in tp]\n",
    "    trainneg=[x[1].split() for x in tp]\n",
    "     \n",
    "    ppp=list(itertools.chain(*(trainpos)))\n",
    "    nnn=list(itertools.chain(*(trainneg)))\n",
    "    #print(ppp)\n",
    "    \n",
    "    \n",
    "    if len(tr)==4:\n",
    "        tp=[x.split('#TAB#') for x in tr[3].split('#N#')]\n",
    "        testpos=[x[0].split() for x in tp]\n",
    "        testneg=[x[1].split() for x in tp]\n",
    "        \n",
    "        \n",
    "        for i in range(len(testpos)):\n",
    "            tp=[]\n",
    "            tp.append(len(all_test_pn))\n",
    "            qqq=list(set(ppp))\n",
    "            allpos=[int(p) for p in random.sample(qqq,min(50,len(qqq)))[:50]]\n",
    "            allpos+=[0]*(50-len(allpos))\n",
    "    \n",
    "            \n",
    "            for j in testpos[i]:\n",
    "                all_test_pn.append(int(j))\n",
    "                all_test_labeler.append(1)\n",
    "                all_test_id.append(userid_dict[userid])\n",
    "                all_test_user_pos.append(allpos)\n",
    "                \n",
    "            for j in testneg[i]:\n",
    "                all_test_pn.append(int(j))\n",
    "                all_test_labeler.append(0)\n",
    "                all_test_id.append(userid_dict[userid])\n",
    "                all_test_user_pos.append(allpos)\n",
    "            tp.append(len(all_test_pn))\n",
    "            all_test_index.append(tp)\n",
    "            \n",
    "    #nnnnn=list(set(nnn)-set(ppp))\n",
    "\n",
    "            \n",
    "    for mp in range(len(trainpos)):\n",
    "        for ps in trainpos[mp]:\n",
    "            #tql=list(set(nnn)-set(trainpos[mp]))\n",
    "            negps=newsample(trainneg[mp],npratio)\n",
    "            negps.append(ps)\n",
    "            tplb=[0]*npratio+[1]\n",
    "            tid=list(range(npratio+1))\n",
    "            random.shuffle(tid)\n",
    "\n",
    "            \n",
    "            yp=[]\n",
    "            yplb=[]\n",
    "            for j in tid:\n",
    "                yp.append(int(negps[j]))\n",
    "                yplb.append(tplb[j])\n",
    "            \n",
    "            qqq=list(set(ppp)-set([ps]))\n",
    "            allpos=[int(p) for p in random.sample(qqq,min(50,len(qqq)))[:50]]\n",
    "            allpos+=[0]*(50-len(allpos))\n",
    "            all_train_pn.append(yp)\n",
    "            all_labeler.append(yplb)\n",
    "            all_train_id.append(userid_dict[userid])\n",
    "            all_user_pos.append(allpos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data_random(all_train_pn,all_labeler,all_train_id,batch_size):\n",
    "    idx = np.arange(len(all_labeler))\n",
    "    np.random.shuffle(idx)\n",
    "    y=all_labeler\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            itx = news_words[all_train_pn[i]]\n",
    "            tkk=all_user_pos[all_train_id[i]]\n",
    "            itxx=[itx[:,k,:] for k in range(itx.shape[1])]\n",
    "            usx=news_words[all_user_pos[i]]\n",
    "            usxx=[usx[:,k,:] for k in range(usx.shape[1])]\n",
    "            uid=np.expand_dims(all_train_id[i],axis=1)\n",
    "            yy=all_labeler[i]\n",
    "            yield (itxx +usxx+[uid], yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data(all_train_pn,all_labeler,all_train_id,batch_size):\n",
    "    idx = np.arange(len(all_labeler))\n",
    "    y=all_labeler\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            itx = news_words[all_train_pn[i]]\n",
    "            usx=news_words[all_test_user_pos[i]]\n",
    "            usxx=[usx[:,k,:] for k in range(usx.shape[1])]\n",
    "            uid=np.expand_dims(all_train_id[i],axis=1)\n",
    "            yy=all_labeler[i]\n",
    "            yield ([itx]+ usxx+[uid], yy)\n",
    "            \n",
    "all_train_pn=np.array(all_train_pn,dtype='int32')\n",
    "all_labeler=np.array(all_labeler,dtype='int32')\n",
    "all_train_id=np.array(all_train_id,dtype='int32')\n",
    "all_test_pn=np.array(all_test_pn,dtype='int32')\n",
    "all_test_labeler=np.array(all_test_labeler,dtype='int32')\n",
    "all_test_id=np.array(all_test_id,dtype='int32')\n",
    "all_user_pos=np.array(all_user_pos,dtype='int32')\n",
    "all_test_user_pos=np.array(all_test_user_pos,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding, concatenate\n",
    "from keras.layers import Dense, Input, Flatten, average,Lambda\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers #keras2\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.optimizers import *\n",
    "import keras\n",
    "import itertools\n",
    "import random\n",
    "results=[]\n",
    "for npratio in range(4,5):\n",
    "    \n",
    "    \n",
    "    MAX_SENT_LENGTH=30\n",
    "    MAX_SENTS=50\n",
    "    \n",
    "    \n",
    "    user_id = Input(shape=(1,), dtype='int32')\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    user_embedding_layer= Embedding(len(user), 50,trainable=True)\n",
    "    \n",
    "    user_embedding= user_embedding_layer(user_id)\n",
    "    \n",
    "    user_embedding0= Dense(200,activation='relu')(user_embedding)\n",
    "    user_embedding0= Flatten()(user_embedding0)\n",
    "    \n",
    "    user_embedding1= Dense(200,activation='relu')(user_embedding)\n",
    "    user_embedding1= Flatten()(user_embedding1)\n",
    "    \n",
    "    \n",
    "    #user_embedding= RepeatVector(1+npratio)(user_embedding0)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    \n",
    "    \n",
    "    embedding_layer = Embedding(33050 , 300, weights=[lister],trainable=True)\n",
    "    \n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    d_et=Dropout(0.2)(embedded_sequences)\n",
    "    \n",
    "    l_cnnt = Convolution1D(nb_filter=400, filter_length=3,  padding='same', activation='relu', strides=1)(d_et)\n",
    "    d_ct=Dropout(0.2)(l_cnnt)\n",
    "    \n",
    "    attention = Dense(200,activation='tanh')(d_ct)\n",
    "    attention = Dot((2, 1))([attention, user_embedding0])\n",
    "    attention_weight = Activation('softmax')(attention)\n",
    "    l_attt=keras.layers.Dot((1, 1))([d_ct, attention_weight])\n",
    "    \n",
    "    \n",
    "    sentEncodert = Model([sentence_input,user_id], l_attt)\n",
    "    \n",
    "    \n",
    "    review_input = [keras.Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(MAX_SENTS)]\n",
    "    candidate_vecssent = [sentEncodert([review,user_id]) for review in review_input]\n",
    "    candidate_vecssent2 =concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(review) for review in candidate_vecssent],axis=1)\n",
    "    #review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "    #review_encoder = TimeDistributed(sentEncodert)(review_input)\n",
    "    #print(K.int_shape(candidate_vecssent2))\n",
    "    \n",
    "    \n",
    "    attention2 = Dense(200,activation='tanh')(candidate_vecssent2)\n",
    "    attention2 = keras.layers.Dot((2, 1))([attention2, user_embedding1])\n",
    "    attention_weight2 = Activation('softmax')(attention2)\n",
    "    l_att2=keras.layers.Dot((1, 1))([candidate_vecssent2, attention_weight2])\n",
    "    #l_att2=AttLayer()(candidate_vecssent2)\n",
    "    \n",
    "    candidates = [keras.Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(1+npratio)]\n",
    "    candidate_vecs = [sentEncodert([candidate,user_id]) for candidate in candidates]\n",
    "    \n",
    "    \n",
    "    logits = [keras.layers.dot([l_att2, candidate_vec], axes=-1) for candidate_vec in candidate_vecs]\n",
    "    logits = keras.layers.Activation(keras.activations.softmax)(keras.layers.concatenate(logits))\n",
    "    \n",
    "    \n",
    "    model = Model(candidates+review_input+[user_id], logits)\n",
    "    \n",
    "    \n",
    "    candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "    candidate_one_vec = sentEncodert([candidate_one,user_id])\n",
    "    score = keras.layers.Activation(keras.activations.sigmoid)(\n",
    "        keras.layers.dot([l_att2, candidate_one_vec], axes=-1))\n",
    "    model2 = keras.Model([candidate_one]+review_input+[user_id], score)\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "\n",
    "    for ep in range(3):\n",
    "        traingen=generate_batch_data_random(all_train_pn,all_labeler,all_train_id, 100)\n",
    "        \n",
    "        model.fit_generator(traingen, epochs=1,steps_per_epoch=len(all_train_id)//100)\n",
    "        #for ep in range(1):    \n",
    "        testgen=generate_batch_data(all_test_pn,all_test_labeler,all_test_id, 100)\n",
    "        cr = model2.predict_generator(testgen, steps=len(all_test_id)//100,verbose=1)\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        all_auc=[]\n",
    "        all_mrr=[]\n",
    "        all_ndcg=[]\n",
    "        all_ndcg2=[]\n",
    "        for m in all_test_index:\n",
    "            if np.sum(all_test_labeler[m[0]:m[1]])!=0 and m[1]<len(cr):\n",
    "        \n",
    "                all_auc.append(roc_auc_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0]))\n",
    "                all_mrr.append(mrr_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0]))\n",
    "                all_ndcg.append(ndcg_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0],k=5))\n",
    "                all_ndcg2.append(ndcg_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0],k=10))\n",
    "        results.append([np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2)])\n",
    "        print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
